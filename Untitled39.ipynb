{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMBPcxz8peJB"
      },
      "outputs": [],
      "source": [
        "# dermai_skin_disease_classifier_final.py\n",
        "\n",
        "# 1. Import Required Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, cv2, matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Load Dataset\n",
        "# -------------------------------\n",
        "IMAGE_DIRS = ['HAM10000_images_part_1', 'HAM10000_images_part_2']  # both folders\n",
        "CSV_PATH = 'HAM10000_metadata.csv'\n",
        "\n",
        "metadata = pd.read_csv(CSV_PATH)\n",
        "print(\"[INFO] Class distribution:\\n\", metadata['dx'].value_counts())\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Preprocess Images\n",
        "# -------------------------------\n",
        "images, labels = [], []\n",
        "\n",
        "for _, row in metadata.iterrows():\n",
        "    img_found = False\n",
        "    for folder in IMAGE_DIRS:\n",
        "        img_path = os.path.join(folder, row['image_id'] + '.jpg')\n",
        "        if os.path.exists(img_path):\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, (224, 224))\n",
        "                images.append(img)\n",
        "                labels.append(row['dx'])\n",
        "                img_found = True\n",
        "                break\n",
        "    if not img_found:\n",
        "        print(f\"[WARNING] Image {row['image_id']} not found!\")\n",
        "\n",
        "images = np.array(images) / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Encode Labels\n",
        "# -------------------------------\n",
        "le = LabelEncoder()\n",
        "labels_encoded = le.fit_transform(labels)\n",
        "labels_categorical = to_categorical(labels_encoded, num_classes=len(le.classes_))\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Train-Test Split\n",
        "# -------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    images, labels_categorical, test_size=0.2, stratify=labels_categorical, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Data Augmentation\n",
        "# -------------------------------\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20, width_shift_range=0.1, height_shift_range=0.1,\n",
        "    horizontal_flip=True, zoom_range=0.1\n",
        ")\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Class Weights (handle imbalance)\n",
        "# -------------------------------\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(labels_encoded),\n",
        "    y=labels_encoded\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(\"[INFO] Class Weights:\", class_weights)\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Custom CNN Model\n",
        "# -------------------------------\n",
        "cnn_model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(le.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# -------------------------------\n",
        "# 9. Transfer Learning ResNet50\n",
        "# -------------------------------\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False  # Freeze\n",
        "\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(len(le.classes_), activation='softmax')(x)\n",
        "\n",
        "resnet_model = Model(inputs=base_model.input, outputs=output)\n",
        "resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# -------------------------------\n",
        "# 10. Callbacks\n",
        "# -------------------------------\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    ModelCheckpoint('best_dermai_model.h5', save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "]\n",
        "\n",
        "# -------------------------------\n",
        "# 11. Train CNN\n",
        "# -------------------------------\n",
        "print(\"[INFO] Training Custom CNN...\")\n",
        "history_cnn = cnn_model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=32),\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=20, callbacks=callbacks,\n",
        "    class_weight=class_weights\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 12. Train ResNet50\n",
        "# -------------------------------\n",
        "print(\"[INFO] Training ResNet50...\")\n",
        "history_resnet = resnet_model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=32),\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10, callbacks=callbacks,\n",
        "    class_weight=class_weights\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 13. Evaluation Function\n",
        "# -------------------------------\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    print(f\"\\nClassification Report ({model_name}):\\n\",\n",
        "          classification_report(y_true, y_pred, target_names=le.classes_))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "    auc_score = roc_auc_score(y_test, y_pred_probs, multi_class='ovr')\n",
        "    print(f\"{model_name} ROC AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "evaluate_model(cnn_model, X_test, y_test, \"Custom CNN\")\n",
        "evaluate_model(resnet_model, X_test, y_test, \"ResNet50\")\n",
        "\n",
        "# -------------------------------\n",
        "# 14. Failure Case Analysis\n",
        "# -------------------------------\n",
        "y_pred = np.argmax(resnet_model.predict(X_test), axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "misclassified_idx = np.where(y_pred != y_true)[0][:9]\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i, idx in enumerate(misclassified_idx):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.imshow(X_test[idx])\n",
        "    plt.title(f\"True: {le.classes_[y_true[idx]]}\\nPred: {le.classes_[y_pred[idx]]}\")\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"Failure Case Analysis (ResNet50)\")\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 15. Save Models\n",
        "# -------------------------------\n",
        "cnn_model.save(\"dermai_cnn_model_final.h5\")\n",
        "resnet_model.save(\"dermai_resnet_model_final.h5\")\n",
        "print(\"[INFO] Models saved successfully!\")\n"
      ]
    }
  ]
}